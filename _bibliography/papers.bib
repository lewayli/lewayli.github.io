---
---

@string{aps = {Large Language Models,}}

@inproceedings{li-etal-2025-fea,
    title = "{FEA}-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation",
    author = "Li, Wei  and
      Zhang, Xin  and
      Guo, Zhongxin  and
      Mao, Shaoguang  and
      Luo, Wen  and
      Peng, Guangyue  and
      Huang, Yangyu  and
      Wang, Houfeng  and
      Li, Scarlett",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.839/",
    pages = "17160--17176",
    ISBN = "979-8-89176-251-0",
    selected={true},
}


@inproceedings{luo-etal-2025-odysseus,
    title = "Odysseus Navigates the Sirens' Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation",
    author = "Luo, Wen  and
      Song, Feifan  and
      Li, Wei  and
      Peng, Guangyue  and
      Wei, Shaohang  and
      Wang, Houfeng",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1320/",
    pages = "27200--27218",
    ISBN = "979-8-89176-251-0"
}

@inproceedings{peng-etal-2025-learn,
    title = "Learn to Memorize: Scalable Continual Learning in Semiparametric Models with Mixture-of-Neighbors Induction Memory",
    author = "Peng, Guangyue  and
      Ge, Tao  and
      Luo, Wen  and
      Li, Wei  and
      Wang, Houfeng",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1385/",
    pages = "28517--28531",
    ISBN = "979-8-89176-251-0"
}

@inproceedings{peng-etal-2025-encode,
    title = "Encode Errors: Representational Retrieval of In-Context Demonstrations for Multilingual Grammatical Error Correction",
    author = "Peng, Guangyue  and
      Li, Wei  and
      Luo, Wen  and
      Wang, Houfeng",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.1090/",
    pages = "21166--21180",
    ISBN = "979-8-89176-256-5"
}

@inproceedings{li-etal-2025-explanation,
    title = "Explanation based In-Context Demonstrations Retrieval for Multilingual Grammatical Error Correction",
    author = "Li, Wei  and
      Luo, Wen  and
      Peng, Guangyue  and
      Wang, Houfeng",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.251/",
    doi = "10.18653/v1/2025.naacl-long.251",
    pages = "4881--4897",
    ISBN = "979-8-89176-189-6",
    selected={true},
}



@inproceedings{li-wang-2024-detection,
    title = "Detection-Correction Structure via General Language Model for Grammatical Error Correction",
    author = "Li, Wei  and
      Wang, Houfeng",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.96/",
    doi = "10.18653/v1/2024.acl-long.96",
    pages = "1748--1763",
    selected={true},
}

@inproceedings{10.1145/3474085.3475585,
author = {Yuan, Ziqi and Li, Wei and Xu, Hua and Yu, Wenmeng},
title = {Transformer-based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475585},
doi = {10.1145/3474085.3475585},
abstract = {Improving robustness against data missing has become one of the core challenges in Multimodal Sentiment Analysis (MSA), which aims to judge speaker sentiments from the language, visual, and acoustic signals. In the current research, translation-based methods and tensor regularization methods are proposed for MSA with incomplete modality features. However, both of them fail to cope with random modality feature missing in non-aligned sequences. In this paper, a transformer-based feature reconstruction network (TFR-Net) is proposed to improve the robustness of models for the random missing in non-aligned modality sequences. First, intra-modal and inter-modal attention-based extractors are adopted to learn robust representations for each element in modality sequences. Then, a reconstruction module is proposed to generate the missing modality features. With the supervision of SmoothL1Loss between generated and complete sequences, TFR-Net is expected to learn semantic-level features corresponding to missing features. Extensive experiments on two public benchmark datasets show that our model achieves good results against data missing across various missing modality combinations and various missing degrees.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {4400â€“4407},
numpages = {8},
keywords = {data missing, feature reconstruction, multimodal sentiment analysis, transformer},
location = {Virtual Event, China},
series = {MM '21},
selected={true},
}